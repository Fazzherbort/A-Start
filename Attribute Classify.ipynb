{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cbt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,log_loss,precision_score, recall_score, f1_score \n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime,timedelta\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\train_set\\first_round_training_data - 副本.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\test_set\\first_round_testing_data - 副本.csv')\n",
    "submit = pd.read_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\submit_example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_map = {'Excellent': 0, 'Good': 1, 'Pass': 2, 'Fail': 3}\n",
    "train['label'] = train['Quality_label'].map(quality_map)\n",
    "# 将标签onehot编码，方便管理和统计。\n",
    "# idea1：可以考虑做4个二分类，或者尝试使用mae 做loss 或者mse 做loss，并观察结果的分布以及线下得分。\n",
    "train = pd.get_dummies(train, columns=['Quality_label'])\n",
    "bin_label = ['Quality_label_Excellent', 'Quality_label_Good', 'Quality_label_Pass', 'Quality_label_Fail']\n",
    "data = pd.concat([train, test], ignore_index=True)\n",
    "data['id'] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为1，属性特征目测是连续特征，2，数值大小分布差异过大，所以将属性做log变换之后做处理，会更合适一些。也可以考虑分桶处理\n",
    "# 为什么做log变换更合理呢？试想一下，假如统计一个人某个数值属性，发现是如下的一个列表[1,2,1.1,1.5,20],\n",
    "# 这种场景分布偏差较大的情况下，如果取均值作为特征，是否合适？\n",
    "# 再比如，如果预测一组数值，一条极端数据对结果的影响超过了N多的数据，这样的模型是否是一个好的模型？\n",
    "# 参数特征这里使用5-10\n",
    "para_feat = ['Parameter{0}'.format(i) for i in range(5, 11)]\n",
    "# 属性特征\n",
    "attr_feat = ['Attribute{0}'.format(i) for i in range(1, 11)]\n",
    "data[attr_feat] = np.log1p(data[attr_feat])\n",
    "# or data[attr_feat] = np.log10(data[attr_feat] + 1)\n",
    "# 此时绘图观察分布显得合理很多\n",
    "#for i in attr_feat:\n",
    "#    data[i].hist()\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用预测属性的model去预测属性，\n",
    "# 1、测试集没有属性，该怎么用？当然是预测它了。预测的方法，可以由模型获得\n",
    "# 2、训练集的属性怎么用？既然测试集的属性是预测出来的，训练集也应该用同等性质的属性，也就是5折交叉预测出来的属性。\n",
    "# 3、第一次想到该方法于2018年的光伏预测赛中，首见成效，之后教与ration（for free），后在icme中，也用到类似方法。\n",
    "# 4、该方法带来的提升目前不太稳定。6750 -- 6800。\n",
    "def get_predict_w(model, data, label='label', feature=[], cate_feature=[], random_state=2018, n_splits=5,model_type='lgb'):\n",
    "    if 'sample_weight' not in data.keys():\n",
    "        data['sample_weight'] = 1\n",
    "    model.random_state = random_state\n",
    "    predict_label = 'predict_' + label\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    data[predict_label] = 0\n",
    "    test_index = (data[label].isnull()) | (data[label] == -1)\n",
    "    train_data = data[~test_index].reset_index(drop=True)\n",
    "    test_data = data[test_index]\n",
    "    for train_idx, val_idx in kfold.split(train_data):\n",
    "        model.random_state = model.random_state + 1\n",
    "        train_x = train_data.loc[train_idx][feature]\n",
    "        train_y = train_data.loc[train_idx][label]\n",
    "        test_x = train_data.loc[val_idx][feature]\n",
    "        test_y = train_data.loc[val_idx][label]\n",
    "        if model_type == 'lgb':\n",
    "            try:\n",
    "                model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=100,\n",
    "                          eval_metric='mae',categorical_feature=cate_feature,\n",
    "                          sample_weight=train_data.loc[train_idx]['sample_weight'],verbose=100)\n",
    "            except:\n",
    "                model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=100,\n",
    "                          eval_metric='mae',sample_weight=train_data.loc[train_idx]['sample_weight'],\n",
    "                          verbose=100)# categorical_feature=cate_feature,        \n",
    "        elif model_type == 'ctb':\n",
    "            model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=100,\n",
    "                      eval_metric='mae',cat_features=cate_feature,\n",
    "                      sample_weight=train_data.loc[train_idx]['sample_weight'],verbose=100)\n",
    "            train_data.loc[val_idx, predict_label] = model.predict(test_x)\n",
    "        if len(test_data) != 0:\n",
    "            test_data[predict_label] = test_data[predict_label] + model.predict(test_data[feature])\n",
    "    test_data[predict_label] = test_data[predict_label] / n_splits\n",
    "    return pd.concat([train_data, test_data], sort=True, ignore_index=True), predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.37377\tvalid_0's l2: 9.52422\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 2.34643\tvalid_0's l2: 9.2934\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.33934\tvalid_0's l2: 8.85975\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 2.31538\tvalid_0's l2: 8.6738\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.19348\tvalid_0's l2: 7.79823\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 2.17014\tvalid_0's l2: 7.60857\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.20048\tvalid_0's l2: 7.81575\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 2.18997\tvalid_0's l2: 7.69738\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.33737\tvalid_0's l2: 9.26249\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 2.30448\tvalid_0's l2: 8.97287\n",
      "predict_Attribute1 done!!\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.29751\tvalid_0's l2: 8.67716\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's l1: 2.27995\tvalid_0's l2: 8.52107\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.34224\tvalid_0's l2: 9.32157\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's l1: 2.33327\tvalid_0's l2: 9.21294\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.36211\tvalid_0's l2: 9.35849\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's l1: 2.34398\tvalid_0's l2: 9.17513\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.41029\tvalid_0's l2: 9.73841\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 2.40108\tvalid_0's l2: 9.63612\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 2.36448\tvalid_0's l2: 9.54581\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's l1: 2.34874\tvalid_0's l2: 9.42884\n",
      "predict_Attribute2 done!!\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.50026\tvalid_0's l2: 3.72605\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's l1: 1.48987\tvalid_0's l2: 3.65744\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.51293\tvalid_0's l2: 3.71496\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's l1: 1.50909\tvalid_0's l2: 3.69205\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.49945\tvalid_0's l2: 3.76015\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's l1: 1.50027\tvalid_0's l2: 3.73127\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.55834\tvalid_0's l2: 4.06997\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's l1: 1.55549\tvalid_0's l2: 4.00683\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.52269\tvalid_0's l2: 3.88776\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's l1: 1.51858\tvalid_0's l2: 3.85083\n",
      "predict_Attribute3 done!!\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.979655\tvalid_0's l2: 2.11264\n",
      "[200]\tvalid_0's l1: 0.936091\tvalid_0's l2: 2.00321\n",
      "[300]\tvalid_0's l1: 0.926872\tvalid_0's l2: 1.97492\n",
      "[400]\tvalid_0's l1: 0.923224\tvalid_0's l2: 1.96423\n",
      "[500]\tvalid_0's l1: 0.923025\tvalid_0's l2: 1.96143\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[459]\tvalid_0's l1: 0.920943\tvalid_0's l2: 1.95988\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.913088\tvalid_0's l2: 1.99107\n",
      "[200]\tvalid_0's l1: 0.882347\tvalid_0's l2: 1.8595\n",
      "[300]\tvalid_0's l1: 0.871066\tvalid_0's l2: 1.8099\n",
      "[400]\tvalid_0's l1: 0.867304\tvalid_0's l2: 1.79659\n",
      "[500]\tvalid_0's l1: 0.865023\tvalid_0's l2: 1.79296\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[493]\tvalid_0's l1: 0.863984\tvalid_0's l2: 1.79169\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.02318\tvalid_0's l2: 2.55027\n",
      "[200]\tvalid_0's l1: 0.977505\tvalid_0's l2: 2.36335\n",
      "[300]\tvalid_0's l1: 0.965696\tvalid_0's l2: 2.31029\n",
      "[400]\tvalid_0's l1: 0.958706\tvalid_0's l2: 2.28644\n",
      "[500]\tvalid_0's l1: 0.9581\tvalid_0's l2: 2.27836\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[468]\tvalid_0's l1: 0.956296\tvalid_0's l2: 2.28103\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.99036\tvalid_0's l2: 2.3071\n",
      "[200]\tvalid_0's l1: 0.950855\tvalid_0's l2: 2.16221\n",
      "[300]\tvalid_0's l1: 0.933862\tvalid_0's l2: 2.07133\n",
      "[400]\tvalid_0's l1: 0.927279\tvalid_0's l2: 2.04033\n",
      "[500]\tvalid_0's l1: 0.922511\tvalid_0's l2: 2.02328\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[486]\tvalid_0's l1: 0.921099\tvalid_0's l2: 2.02169\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.00363\tvalid_0's l2: 2.52262\n",
      "[200]\tvalid_0's l1: 0.979335\tvalid_0's l2: 2.42832\n",
      "[300]\tvalid_0's l1: 0.971514\tvalid_0's l2: 2.38741\n",
      "[400]\tvalid_0's l1: 0.967648\tvalid_0's l2: 2.3673\n",
      "[500]\tvalid_0's l1: 0.967782\tvalid_0's l2: 2.36367\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[452]\tvalid_0's l1: 0.965892\tvalid_0's l2: 2.36516\n",
      "predict_Attribute4 done!!\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.84904\tvalid_0's l2: 1.83816\n",
      "[200]\tvalid_0's l1: 0.808179\tvalid_0's l2: 1.76532\n",
      "[300]\tvalid_0's l1: 0.803502\tvalid_0's l2: 1.76248\n",
      "Early stopping, best iteration is:\n",
      "[247]\tvalid_0's l1: 0.802642\tvalid_0's l2: 1.76018\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.866936\tvalid_0's l2: 1.97019\n",
      "[200]\tvalid_0's l1: 0.818372\tvalid_0's l2: 1.7658\n",
      "[300]\tvalid_0's l1: 0.804423\tvalid_0's l2: 1.69539\n",
      "[400]\tvalid_0's l1: 0.800625\tvalid_0's l2: 1.66239\n",
      "[500]\tvalid_0's l1: 0.797897\tvalid_0's l2: 1.65332\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[492]\tvalid_0's l1: 0.797043\tvalid_0's l2: 1.65323\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.894091\tvalid_0's l2: 2.00599\n",
      "[200]\tvalid_0's l1: 0.845224\tvalid_0's l2: 1.80841\n",
      "[300]\tvalid_0's l1: 0.830575\tvalid_0's l2: 1.75001\n",
      "[400]\tvalid_0's l1: 0.823872\tvalid_0's l2: 1.7286\n",
      "[500]\tvalid_0's l1: 0.821812\tvalid_0's l2: 1.71896\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[468]\tvalid_0's l1: 0.820445\tvalid_0's l2: 1.72013\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.916079\tvalid_0's l2: 2.23547\n",
      "[200]\tvalid_0's l1: 0.872067\tvalid_0's l2: 2.02036\n",
      "[300]\tvalid_0's l1: 0.851523\tvalid_0's l2: 1.9137\n",
      "[400]\tvalid_0's l1: 0.837598\tvalid_0's l2: 1.85193\n",
      "[500]\tvalid_0's l1: 0.832698\tvalid_0's l2: 1.82601\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[486]\tvalid_0's l1: 0.831609\tvalid_0's l2: 1.82477\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.864734\tvalid_0's l2: 2.00058\n",
      "[200]\tvalid_0's l1: 0.846412\tvalid_0's l2: 1.91352\n",
      "[300]\tvalid_0's l1: 0.841761\tvalid_0's l2: 1.87928\n",
      "[400]\tvalid_0's l1: 0.839706\tvalid_0's l2: 1.85913\n",
      "[500]\tvalid_0's l1: 0.83946\tvalid_0's l2: 1.85032\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[422]\tvalid_0's l1: 0.83891\tvalid_0's l2: 1.8568\n",
      "predict_Attribute5 done!!\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.930116\tvalid_0's l2: 1.84315\n",
      "[200]\tvalid_0's l1: 0.902896\tvalid_0's l2: 1.77333\n",
      "[300]\tvalid_0's l1: 0.894694\tvalid_0's l2: 1.74846\n",
      "[400]\tvalid_0's l1: 0.890774\tvalid_0's l2: 1.72834\n",
      "[500]\tvalid_0's l1: 0.887488\tvalid_0's l2: 1.7161\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[488]\tvalid_0's l1: 0.886929\tvalid_0's l2: 1.7156\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.824375\tvalid_0's l2: 1.38602\n",
      "[200]\tvalid_0's l1: 0.799409\tvalid_0's l2: 1.34301\n",
      "[300]\tvalid_0's l1: 0.792795\tvalid_0's l2: 1.32964\n",
      "[400]\tvalid_0's l1: 0.792053\tvalid_0's l2: 1.33038\n",
      "Early stopping, best iteration is:\n",
      "[355]\tvalid_0's l1: 0.79114\tvalid_0's l2: 1.3269\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.847565\tvalid_0's l2: 1.62046\n",
      "[200]\tvalid_0's l1: 0.826274\tvalid_0's l2: 1.57553\n",
      "[300]\tvalid_0's l1: 0.820509\tvalid_0's l2: 1.56158\n",
      "[400]\tvalid_0's l1: 0.815432\tvalid_0's l2: 1.55071\n",
      "[500]\tvalid_0's l1: 0.8153\tvalid_0's l2: 1.54479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[454]\tvalid_0's l1: 0.814383\tvalid_0's l2: 1.54842\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.873775\tvalid_0's l2: 1.58469\n",
      "[200]\tvalid_0's l1: 0.848502\tvalid_0's l2: 1.52482\n",
      "[300]\tvalid_0's l1: 0.841463\tvalid_0's l2: 1.49858\n",
      "[400]\tvalid_0's l1: 0.840252\tvalid_0's l2: 1.49273\n",
      "[500]\tvalid_0's l1: 0.838946\tvalid_0's l2: 1.48806\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[469]\tvalid_0's l1: 0.837289\tvalid_0's l2: 1.48654\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 0.917841\tvalid_0's l2: 2.02409\n",
      "[200]\tvalid_0's l1: 0.900398\tvalid_0's l2: 1.96267\n",
      "[300]\tvalid_0's l1: 0.896537\tvalid_0's l2: 1.93422\n",
      "[400]\tvalid_0's l1: 0.893352\tvalid_0's l2: 1.91474\n",
      "[500]\tvalid_0's l1: 0.891801\tvalid_0's l2: 1.91018\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[497]\tvalid_0's l1: 0.891312\tvalid_0's l2: 1.90931\n",
      "predict_Attribute6 done!!\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.29187\tvalid_0's l2: 3.34376\n",
      "[200]\tvalid_0's l1: 1.2721\tvalid_0's l2: 3.28628\n",
      "[300]\tvalid_0's l1: 1.26781\tvalid_0's l2: 3.28428\n",
      "Early stopping, best iteration is:\n",
      "[244]\tvalid_0's l1: 1.26735\tvalid_0's l2: 3.27919\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.24662\tvalid_0's l2: 3.32186\n",
      "[200]\tvalid_0's l1: 1.2278\tvalid_0's l2: 3.22174\n",
      "[300]\tvalid_0's l1: 1.22325\tvalid_0's l2: 3.18958\n",
      "[400]\tvalid_0's l1: 1.22275\tvalid_0's l2: 3.17604\n",
      "[500]\tvalid_0's l1: 1.22258\tvalid_0's l2: 3.18092\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[412]\tvalid_0's l1: 1.21956\tvalid_0's l2: 3.17383\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.32424\tvalid_0's l2: 4.03993\n",
      "[200]\tvalid_0's l1: 1.30345\tvalid_0's l2: 3.96703\n",
      "[300]\tvalid_0's l1: 1.29558\tvalid_0's l2: 3.95847\n",
      "Early stopping, best iteration is:\n",
      "[256]\tvalid_0's l1: 1.29602\tvalid_0's l2: 3.95233\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.32854\tvalid_0's l2: 3.95824\n",
      "[200]\tvalid_0's l1: 1.3083\tvalid_0's l2: 3.8528\n",
      "[300]\tvalid_0's l1: 1.30219\tvalid_0's l2: 3.79799\n",
      "[400]\tvalid_0's l1: 1.29338\tvalid_0's l2: 3.76084\n",
      "[500]\tvalid_0's l1: 1.29234\tvalid_0's l2: 3.76022\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[479]\tvalid_0's l1: 1.29106\tvalid_0's l2: 3.75949\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.28635\tvalid_0's l2: 3.52399\n",
      "[200]\tvalid_0's l1: 1.27593\tvalid_0's l2: 3.45403\n",
      "[300]\tvalid_0's l1: 1.26974\tvalid_0's l2: 3.44342\n",
      "[400]\tvalid_0's l1: 1.26868\tvalid_0's l2: 3.44428\n",
      "Early stopping, best iteration is:\n",
      "[314]\tvalid_0's l1: 1.26626\tvalid_0's l2: 3.43737\n",
      "predict_Attribute7 done!!\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.2684\tvalid_0's l2: 3.42341\n",
      "[200]\tvalid_0's l1: 1.24505\tvalid_0's l2: 3.41456\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid_0's l1: 1.25333\tvalid_0's l2: 3.40817\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.28362\tvalid_0's l2: 3.6816\n",
      "[200]\tvalid_0's l1: 1.2604\tvalid_0's l2: 3.5592\n",
      "[300]\tvalid_0's l1: 1.25371\tvalid_0's l2: 3.52891\n",
      "[400]\tvalid_0's l1: 1.252\tvalid_0's l2: 3.51051\n",
      "Early stopping, best iteration is:\n",
      "[379]\tvalid_0's l1: 1.2516\tvalid_0's l2: 3.51013\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.29206\tvalid_0's l2: 3.44166\n",
      "[200]\tvalid_0's l1: 1.25581\tvalid_0's l2: 3.32324\n",
      "[300]\tvalid_0's l1: 1.24544\tvalid_0's l2: 3.30167\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid_0's l1: 1.24546\tvalid_0's l2: 3.29918\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.31815\tvalid_0's l2: 3.86628\n",
      "[200]\tvalid_0's l1: 1.29967\tvalid_0's l2: 3.76085\n",
      "[300]\tvalid_0's l1: 1.29499\tvalid_0's l2: 3.71941\n",
      "[400]\tvalid_0's l1: 1.28905\tvalid_0's l2: 3.70163\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's l1: 1.28694\tvalid_0's l2: 3.69952\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.29936\tvalid_0's l2: 3.86426\n",
      "[200]\tvalid_0's l1: 1.28885\tvalid_0's l2: 3.77511\n",
      "[300]\tvalid_0's l1: 1.28395\tvalid_0's l2: 3.74454\n",
      "[400]\tvalid_0's l1: 1.28154\tvalid_0's l2: 3.73319\n",
      "[500]\tvalid_0's l1: 1.28188\tvalid_0's l2: 3.7414\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[452]\tvalid_0's l1: 1.27778\tvalid_0's l2: 3.72905\n",
      "predict_Attribute8 done!!\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.22153\tvalid_0's l2: 3.3482\n",
      "[200]\tvalid_0's l1: 1.20789\tvalid_0's l2: 3.28956\n",
      "[300]\tvalid_0's l1: 1.2052\tvalid_0's l2: 3.27831\n",
      "Early stopping, best iteration is:\n",
      "[290]\tvalid_0's l1: 1.20356\tvalid_0's l2: 3.28192\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.23706\tvalid_0's l2: 3.35587\n",
      "[200]\tvalid_0's l1: 1.22891\tvalid_0's l2: 3.3252\n",
      "Early stopping, best iteration is:\n",
      "[155]\tvalid_0's l1: 1.22573\tvalid_0's l2: 3.3228\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.26417\tvalid_0's l2: 3.57699\n",
      "[200]\tvalid_0's l1: 1.25313\tvalid_0's l2: 3.53909\n",
      "[300]\tvalid_0's l1: 1.25355\tvalid_0's l2: 3.53112\n",
      "Early stopping, best iteration is:\n",
      "[246]\tvalid_0's l1: 1.24925\tvalid_0's l2: 3.52761\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.25776\tvalid_0's l2: 3.27234\n",
      "[200]\tvalid_0's l1: 1.24459\tvalid_0's l2: 3.22824\n",
      "[300]\tvalid_0's l1: 1.24424\tvalid_0's l2: 3.23563\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's l1: 1.24238\tvalid_0's l2: 3.22296\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.29563\tvalid_0's l2: 3.79422\n",
      "[200]\tvalid_0's l1: 1.286\tvalid_0's l2: 3.78584\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid_0's l1: 1.29044\tvalid_0's l2: 3.77187\n",
      "predict_Attribute9 done!!\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.23407\tvalid_0's l2: 3.47772\n",
      "[200]\tvalid_0's l1: 1.22635\tvalid_0's l2: 3.44628\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's l1: 1.22755\tvalid_0's l2: 3.44288\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.24359\tvalid_0's l2: 3.31328\n",
      "[200]\tvalid_0's l1: 1.23359\tvalid_0's l2: 3.23477\n",
      "[300]\tvalid_0's l1: 1.23111\tvalid_0's l2: 3.2134\n",
      "[400]\tvalid_0's l1: 1.23261\tvalid_0's l2: 3.21194\n",
      "Early stopping, best iteration is:\n",
      "[328]\tvalid_0's l1: 1.22766\tvalid_0's l2: 3.20803\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.25492\tvalid_0's l2: 3.32009\n",
      "[200]\tvalid_0's l1: 1.24101\tvalid_0's l2: 3.25194\n",
      "[300]\tvalid_0's l1: 1.23974\tvalid_0's l2: 3.22992\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid_0's l1: 1.23582\tvalid_0's l2: 3.22808\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.30556\tvalid_0's l2: 3.9413\n",
      "[200]\tvalid_0's l1: 1.29346\tvalid_0's l2: 3.87\n",
      "[300]\tvalid_0's l1: 1.29076\tvalid_0's l2: 3.83815\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid_0's l1: 1.28712\tvalid_0's l2: 3.84663\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l1: 1.24545\tvalid_0's l2: 3.52126\n",
      "[200]\tvalid_0's l1: 1.23805\tvalid_0's l2: 3.46692\n",
      "[300]\tvalid_0's l1: 1.23656\tvalid_0's l2: 3.46239\n",
      "[400]\tvalid_0's l1: 1.23601\tvalid_0's l2: 3.45149\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalid_0's l1: 1.23624\tvalid_0's l2: 3.4498\n",
      "predict_Attribute10 done!!\n"
     ]
    }
   ],
   "source": [
    "lgb_attr_model = lgb.LGBMRegressor(boosting_type=\"gbdt\", num_leaves=31, reg_alpha=10, reg_lambda=5,\n",
    "                                   max_depth=7, n_estimators=500,subsample=0.7, colsample_bytree=0.4, \n",
    "                                   subsample_freq=2, min_child_samples=10,learning_rate=0.05, random_state=2019,)\n",
    "features = para_feat\n",
    "for i in attr_feat:\n",
    "    data, predict_label = get_predict_w(lgb_attr_model, data, label=i,feature=features, random_state=2019, n_splits=5)\n",
    "    print(predict_label, 'done!!')\n",
    "# 该方案共获得10个属性特征。\n",
    "pred_attr_feat = ['predict_Attribute{0}'.format(i) for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 16) (6000, 16)\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.239479984100655\n",
      "ac 0.5068333333333334\n",
      "mae 0.06658805119623377\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.269550215100713\n",
      "ac 0.5036666666666667\n",
      "mae 0.06680875283835168\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.2569207008868917\n",
      "ac 0.5015\n",
      "mae 0.06672830837792856\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.249085385095412\n",
      "ac 0.5075\n",
      "mae 0.06677955916246239\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.2593278791629172\n",
      "ac 0.49866666666666665\n",
      "mae 0.06675208495447135\n",
      "logloss 1.1894041102609556\n",
      "ac 0.5151666666666667\n",
      "mae 0.06673126341778068\n",
      "micro precision 0.5151666666666667\n",
      "micro recall 0.5151666666666667\n",
      "macro precision 0.4816454908973167\n",
      "macro recall 0.4695703693172557\n",
      "micro F2-Score 0.5151666666666667\n",
      "macro F2-Score 0.47193671273458854\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "boosttype = 'xgb'\n",
    "feature_name = pred_attr_feat + para_feat#选取特征\n",
    "tr_index = ~data['label'].isnull()#将data中label这一列不为空的行数选取出来\n",
    "data[tr_index][pred_attr_feat] = data[tr_index][attr_feat]\n",
    "X_train = data[tr_index][feature_name].reset_index(drop=True)#用tr_index选出trainset\n",
    "y = data[tr_index]['label'].reset_index(drop=True).astype(int)#label of trainset\n",
    "X_test = data[~tr_index][feature_name].reset_index(drop=True)#用tr_index的取反得到testset\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "oof = np.zeros((X_train.shape[0],4))\n",
    "prediction = np.zeros((X_test.shape[0],4))\n",
    "seeds = [x*x for x in range(100,200,20)]#设置随机种子\n",
    "num_model_seed = len(seeds)\n",
    "#params = {'max_depth':range(2, 7), 'n_estimators':range(100, 1100, 200), 'learning_rate':[0.01]}\n",
    "\n",
    "for model_seed in range(num_model_seed):\n",
    "    print(model_seed + 1)\n",
    "    oof_model = np.zeros((X_train.shape[0],4))\n",
    "    prediction_model=np.zeros((X_test.shape[0],4))\n",
    "#    oof_xgb = np.zeros((X_train.shape[0],4))\n",
    "#    prediction_xgb=np.zeros((X_test.shape[0],4))\n",
    "    skf = StratifiedKFold(n_splits=k, random_state=seeds[model_seed], shuffle=True)#构造交叉验证集，进行按比例取样（splits表示测试集和验证集为4：1）\n",
    "    for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n",
    "        print(index)\n",
    "        train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "        gc.collect()#进行垃圾内存回收\n",
    "        if boosttype=='cbt':\n",
    "            model = cbt.CatBoostClassifier(iterations=4000,learning_rate=0.05,verbose=1000,max_depth=6,\n",
    "                                           early_stopping_rounds=2800,\n",
    "                                           task_type='GPU',loss_function='MultiClass')\n",
    "            model.fit(train_x, train_y ,eval_set=(train_x, train_y))\n",
    "        elif boosttype=='xgb':\n",
    "            model = XGBClassifier(\n",
    "            silent=True ,#设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "            #nthread=4,# cpu 线程数 默认最大\n",
    "            learning_rate= 0.05, # 如同学习率\n",
    "            min_child_weight=1,\n",
    "            # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "            #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "            #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "            max_depth=8, # 构建树的深度，越大越容易过拟合\n",
    "            gamma=0.1,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "            subsample=1, # 随机采样训练样本 训练实例的子采样比\n",
    "            max_delta_step=0,#最大增量步长，我们允许每个树的权重估计。\n",
    "            colsample_bytree=1, # 生成树时进行的列采样\n",
    "            reg_lambda=1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "            #reg_alpha=0, # L1 正则项参数\n",
    "            scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n",
    "            objective= 'multi:softmax', #多分类的问题 指定学习任务和相应的学习目标\n",
    "            num_class=4, # 类别数，多分类与 multisoftmax 并用\n",
    "            n_estimators=2000, #树的个数\n",
    "            seed=2019, #随机种子\n",
    "            eval_metric= 'mlogloss',\n",
    "            iterations=2000,\n",
    "            verbose=1000\n",
    "            )\n",
    "            model.fit(train_x, train_y,eval_metric='mlogloss')\n",
    "        elif boosttype=='lgb':\n",
    "            model=lgb.LGBMClassifier(boosting_type=\"gbdt\",num_leaves=23,reg_alpha=10, reg_lambda=5,max_depth=7,\n",
    "                                 learning_rate=0.05, n_estimators=2000,colsample_bytree=0.7, subsample_freq=1,\n",
    "                                 objective='multiclass', silent=True,subsample=0.7,min_child_samples=5,\n",
    "                                 #reg_alpha=1.,bagging_fraction=0.9, feature_fraction=0.9,\n",
    "                                 )\n",
    "            model.fit(train_x, train_y, eval_set=(test_x, test_y),early_stopping_rounds=3000, verbose=1000,)#verbose=False   \n",
    "        oof_model[test_index] = oof_model[test_index]+model.predict_proba(test_x) #预测验证集上某一个工件lebal为0到3的各自概率\n",
    "        prediction_model = prediction_model+model.predict_proba(X_test)/k #预测测试集上某一个工件lebal为0到3的各自概率（除10是因为splits=10，故要进行10次轮换交叉验证）\n",
    "    oof = oof+oof_model / num_model_seed\n",
    "    prediction =prediction+prediction_model / num_model_seed\n",
    "\n",
    "    print('logloss',log_loss(pd.get_dummies(y).values, oof_model))\n",
    "    print('ac',accuracy_score(y, np.argmax(oof_model,axis=1)))\n",
    "    print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y] - oof_model))/480))\n",
    "\n",
    "print('logloss',log_loss(pd.get_dummies(y).values, oof))\n",
    "print('ac',accuracy_score(y, np.argmax(oof,axis=1)))\n",
    "print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y] - oof))/480))\n",
    "beta=2\n",
    "p1=precision_score(y,np.argmax(oof,axis=1),average='micro')\n",
    "r1=recall_score(y,np.argmax(oof,axis=1),average='micro')\n",
    "p2=precision_score(y,np.argmax(oof,axis=1),average='macro')\n",
    "r2=recall_score(y,np.argmax(oof,axis=1),average='macro')\n",
    "print('micro precision',p1)\n",
    "print('micro recall',r1)\n",
    "print('macro precision',p2)\n",
    "print('macro recall',r2)\n",
    "print('micro F2-Score',(1+beta*beta)*p1*r1/(beta*beta*p1+r1))\n",
    "print('macro F2-Score',(1+beta*beta)*p2*r2/(beta*beta*p2+r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 16) (6000, 16)\n",
      "0:\tlearn: -1.3813354\ttest: -1.3813356\tbest: -1.3813356 (0)\ttotal: 19.1ms\tremaining: 38.2s\n",
      "1000:\tlearn: -1.0310777\ttest: -1.0310777\tbest: -1.0310777 (1000)\ttotal: 12.8s\tremaining: 12.8s\n",
      "1999:\tlearn: -0.9836363\ttest: -0.9836363\tbest: -0.9836363 (1999)\ttotal: 26s\tremaining: 0us\n",
      "bestTest = -0.9836363118\n",
      "bestIteration = 1999\n",
      "Shrink model to first 2000 iterations.\n",
      "logloss 0.9836364055023246\n",
      "ac 0.5875\n",
      "mae 0.06659954131602225\n",
      "micro precision 0.5875\n",
      "micro recall 0.5875\n",
      "macro precision 0.5793030446693477\n",
      "macro recall 0.536193338552185\n",
      "micro F2-Score 0.5875\n",
      "macro F2-Score 0.54429423311743\n"
     ]
    }
   ],
   "source": [
    "feature_name = pred_attr_feat + para_feat#选取特征\n",
    "tr_index = ~data['label'].isnull()#将data中label这一列不为空的行数选取出来\n",
    "data[tr_index][pred_attr_feat] = data[tr_index][attr_feat]\n",
    "X_train = data[tr_index][feature_name].reset_index(drop=True)#用tr_index选出trainset\n",
    "y = data[tr_index]['label'].reset_index(drop=True).astype(int)#label of trainset\n",
    "#y = data[tr_index]['label'].reset_index(drop=True).astype(int)#label of trainset\n",
    "#y = data[tr_index]['Attribute1'].reset_index(drop=True).astype(float)\n",
    "X_test = data[~tr_index][feature_name].reset_index(drop=True)#用tr_index的取反得到testset\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "oof = np.zeros((X_train.shape[0],4))\n",
    "prediction = np.zeros((X_test.shape[0],4))\n",
    "#seeds = [1000,3000,10000,30000,100000]#设置随机种子\n",
    "#num_model_seed = 9\n",
    "#params = {'max_depth':range(2, 7), 'n_estimators':range(100, 1100, 200), 'learning_rate':[0.01]}\n",
    "#for model_seed in range(num_model_seed):\n",
    "#    print(model_seed + 1)\n",
    "#    oof_cat = np.zeros((X_train.shape[0],4))\n",
    "#    prediction_cat=np.zeros((X_test.shape[0],4))\n",
    "#    oof_xgb = np.zeros((X_train.shape[0],4))\n",
    "#    prediction_xgb=np.zeros((X_test.shape[0],4))\n",
    "#    skf = StratifiedKFold(n_splits=5, random_state=seeds[model_seed], shuffle=True)#构造交叉验证集，进行按比例取样（splits=10表示测试集和验证集为4：1）\n",
    "#    for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n",
    "#        print(index)\n",
    "#        train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "#        gc.collect()#进行垃圾内存回收\n",
    "cbt_model = cbt.CatBoostClassifier(iterations=2000,learning_rate=0.01,verbose=1000,\n",
    "                                   early_stopping_rounds=1800,task_type='GPU',loss_function='MultiClass',\n",
    "                                   ) #loss_function='MultiClass'\n",
    "#        xgb_model = GridSearchCV(XGBClassifier(),params,n_jobs=-1, cv=5, verbose=300,refit=True)\n",
    "#        cbt_model.fit(train_x, train_y ,eval_set=(train_x, train_y))#求一些方差、标准差、最大值和最小值\n",
    "#cbt_model.fit(X_train, y ,eval_set=(X_train, y))#求一些方差、标准差、最大值和最小值    \n",
    "cbt_model.fit(X_train, y,eval_set=(X_train, y))#求一些方差、标准差、最大值和最小值\n",
    "    #        oof_cat[test_index] += cbt_model.predict_proba(test_x)#预测验证集上某一个工件lebal为0到3的各自概率\n",
    "    #        prediction_cat += cbt_model.predict_proba(X_test)/10#预测测试集上某一个工件lebal为0到3的各自概率（除10是因为splits=10，故要进行10次轮换交叉验证）\n",
    "    #    oof += oof_cat / num_model_seed\n",
    "    #    prediction += prediction_cat / num_model_seed\n",
    "    #        xgb_model.fit(train_x, train_y)\n",
    "    #        oof_xgb[test_index] += xgb_model.predict_proba(test_x)\n",
    "    #        prediction_xgb += xgb_model.predict_proba(X_test)/5\n",
    "    #    oof += oof_xgb / num_model_seed\n",
    "    #    prediction += prediction_xgb / num_model_seed\n",
    "    #    print('logloss',log_loss(pd.get_dummies(y).values, oof_cat))\n",
    "    #    print('ac',accuracy_score(y, np.argmax(oof_cat,axis=1)))\n",
    "    #    print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y] - oof_cat))/480))\n",
    "oof = cbt_model.predict_proba(X_train)\n",
    "prediction = cbt_model.predict_proba(X_test)\n",
    "#oof = cbt_model.predict(X_train)\n",
    "#prediction = cbt_model.predict(X_test)\n",
    "gc.collect()\n",
    "#print('logloss',log_loss(pd.get_dummies(y).values, oof))\n",
    "#print('ac',accuracy_score(y, np.argmax(oof,axis=1)))\n",
    "#print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y] - oof))/480))\n",
    "print('logloss',log_loss(pd.get_dummies(y).values, oof))\n",
    "print('ac',accuracy_score(y, np.argmax(oof,axis=1)))\n",
    "print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y] - oof))/480))\n",
    "beta=2\n",
    "p1=precision_score(y,np.argmax(oof,axis=1),average='micro')\n",
    "r1=recall_score(y,np.argmax(oof,axis=1),average='micro')\n",
    "p2=precision_score(y,np.argmax(oof,axis=1),average='macro')\n",
    "r2=recall_score(y,np.argmax(oof,axis=1),average='macro')\n",
    "print('micro precision',p1)\n",
    "print('micro recall',r1)\n",
    "print('macro precision',p2)\n",
    "print('macro recall',r2)\n",
    "print('micro F2-Score',(1+beta*beta)*p1*r1/(beta*beta*p1+r1))\n",
    "print('macro F2-Score',(1+beta*beta)*p2*r2/(beta*beta*p2+r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test[['Group']]#将predications写入到submit_example中\n",
    "prob_cols = [i for i in submit.columns if i not in ['Group']]\n",
    "for i, f in enumerate(prob_cols):\n",
    "    sub[f] = prediction[:, i]\n",
    "for i in prob_cols:\n",
    "    sub[i] = sub.groupby('Group')[i].transform('mean')\n",
    "sub = sub.drop_duplicates()\n",
    "#sub.to_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\submit_example.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = sub.copy()\n",
    "submit2 = sub.copy()\n",
    "submit3 = sub.copy()\n",
    "ratio = ['Excellent ratio','Good ratio','Pass ratio','Fail ratio']\n",
    "submit[ratio]=submit[ratio]*50\n",
    "submit2[ratio]=(submit2[ratio]*50).astype('int')\n",
    "submit3[ratio]=(submit[ratio]-submit2[ratio]).astype('float')\n",
    "submit2 = submit2.values\n",
    "submit3 = submit3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.argmax(submit3[:,1:5],axis=1) + 1\n",
    "for i in range(120): \n",
    "    if submit3[i,k[i]] > 0.5:\n",
    "        submit2[i,k[i]] += 1\n",
    "        submit3[i,k[i]] = 0\n",
    "k = np.argmax(submit3[:,1:5],axis=1) + 1\n",
    "for i in range(120):\n",
    "    if submit3[i,k[i]] > 0.5:\n",
    "        submit2[i,k[i]] += 1\n",
    "        submit3[i,k[i]] = 0\n",
    "k = np.argmax(submit3[:,1:5],axis=1) + 1\n",
    "for i in range(120):    \n",
    "    if sum(submit2[i,1:5]) < 50:\n",
    "        if submit3[i,k[i]] > 0.5:\n",
    "            submit2[i,k[i]] += 1\n",
    "            submit3[i,k[i]] = 0\n",
    "k = np.argmax(submit3[:,1:5],axis=1) + 1\n",
    "for i in range(120):    \n",
    "    if sum(submit2[i,1:5]) < 50:\n",
    "        submit2[i,k[i]] += 1\n",
    "        submit3[i,k[i]] = 0\n",
    "submit[ratio] = submit2[:,1:5] / 50\n",
    "#submit.to_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\submit_example.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [ 0.  ,  0.02,  0.  , -0.02],\n",
       "       [-0.02,  0.04,  0.  , -0.02],\n",
       "       [ 0.  ,  0.02, -0.04,  0.02],\n",
       "       [ 0.02,  0.02, -0.04,  0.  ],\n",
       "       [ 0.  ,  0.02,  0.  , -0.02],\n",
       "       [ 0.02, -0.02, -0.04,  0.04],\n",
       "       [ 0.04,  0.  , -0.04,  0.  ],\n",
       "       [ 0.  , -0.02,  0.02,  0.  ],\n",
       "       [ 0.02,  0.02, -0.02, -0.02],\n",
       "       [ 0.02,  0.  , -0.04,  0.02],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.06, -0.04, -0.02,  0.  ],\n",
       "       [ 0.02,  0.  ,  0.  , -0.02],\n",
       "       [ 0.06, -0.08,  0.  ,  0.02],\n",
       "       [ 0.04, -0.02, -0.02,  0.  ],\n",
       "       [ 0.02, -0.04,  0.  ,  0.02],\n",
       "       [ 0.02, -0.02, -0.02,  0.02],\n",
       "       [ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [ 0.02,  0.02, -0.04,  0.  ],\n",
       "       [ 0.  ,  0.02,  0.  , -0.02],\n",
       "       [ 0.  ,  0.02, -0.02,  0.  ],\n",
       "       [ 0.  , -0.02, -0.02,  0.04],\n",
       "       [ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [ 0.02, -0.02, -0.02,  0.02],\n",
       "       [ 0.  ,  0.02,  0.  , -0.02],\n",
       "       [ 0.04, -0.04,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.04, -0.04],\n",
       "       [ 0.06, -0.06,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.02, -0.02,  0.  ],\n",
       "       [ 0.04, -0.06,  0.02,  0.  ],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.02, -0.02],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.06, -0.06],\n",
       "       [ 0.02,  0.04, -0.06,  0.  ],\n",
       "       [-0.02,  0.02, -0.02,  0.02],\n",
       "       [ 0.04,  0.02, -0.04, -0.02],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.04,  0.  , -0.04,  0.  ],\n",
       "       [-0.06,  0.06, -0.02,  0.02],\n",
       "       [ 0.02,  0.02, -0.06,  0.02],\n",
       "       [-0.02,  0.04, -0.04,  0.02],\n",
       "       [ 0.  , -0.02,  0.  ,  0.02],\n",
       "       [ 0.02,  0.04, -0.06,  0.  ],\n",
       "       [-0.02,  0.  ,  0.  ,  0.02],\n",
       "       [ 0.04,  0.02, -0.06,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.06,  0.02, -0.08,  0.  ],\n",
       "       [-0.02,  0.  ,  0.02,  0.  ],\n",
       "       [ 0.  ,  0.02, -0.02,  0.  ],\n",
       "       [ 0.04, -0.02, -0.04,  0.02],\n",
       "       [-0.02,  0.  ,  0.  ,  0.02],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [ 0.02, -0.02, -0.02,  0.02],\n",
       "       [-0.04,  0.  ,  0.04,  0.  ],\n",
       "       [ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [-0.04, -0.04,  0.06,  0.02],\n",
       "       [ 0.  ,  0.02, -0.04,  0.02],\n",
       "       [ 0.  , -0.02,  0.02,  0.  ],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.02,  0.04, -0.08,  0.02],\n",
       "       [ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [ 0.02,  0.  , -0.04,  0.02],\n",
       "       [-0.02,  0.  ,  0.04, -0.02],\n",
       "       [ 0.  ,  0.02, -0.02,  0.  ],\n",
       "       [ 0.06, -0.02,  0.  , -0.04],\n",
       "       [ 0.02,  0.  , -0.04,  0.02],\n",
       "       [ 0.  ,  0.  ,  0.04, -0.04],\n",
       "       [ 0.02,  0.02, -0.04,  0.  ],\n",
       "       [ 0.02,  0.02, -0.08,  0.04],\n",
       "       [ 0.02,  0.  , -0.04,  0.02],\n",
       "       [ 0.02,  0.02,  0.  , -0.04],\n",
       "       [-0.02,  0.02, -0.04,  0.04],\n",
       "       [ 0.  ,  0.02, -0.02,  0.  ],\n",
       "       [ 0.02,  0.  , -0.04,  0.02],\n",
       "       [ 0.02,  0.04, -0.04, -0.02],\n",
       "       [ 0.  ,  0.02, -0.04,  0.02],\n",
       "       [ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [ 0.02,  0.  ,  0.  , -0.02],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.04,  0.02, -0.04, -0.02],\n",
       "       [ 0.02, -0.04, -0.02,  0.04],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.02, -0.04, -0.04,  0.06],\n",
       "       [ 0.04,  0.  , -0.02, -0.02],\n",
       "       [ 0.  , -0.06,  0.  ,  0.06],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.04, -0.04, -0.04,  0.04],\n",
       "       [ 0.  , -0.02,  0.  ,  0.02],\n",
       "       [ 0.  , -0.02,  0.  ,  0.02],\n",
       "       [ 0.02, -0.02, -0.02,  0.02],\n",
       "       [ 0.  ,  0.02, -0.06,  0.04],\n",
       "       [ 0.02,  0.02,  0.  , -0.04],\n",
       "       [ 0.  ,  0.02,  0.04, -0.06],\n",
       "       [ 0.  ,  0.04,  0.  , -0.04],\n",
       "       [ 0.02, -0.02, -0.02,  0.02],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.06,  0.04, -0.02, -0.08],\n",
       "       [ 0.02,  0.  , -0.04,  0.02],\n",
       "       [ 0.  ,  0.08, -0.12,  0.04],\n",
       "       [ 0.04,  0.  , -0.04,  0.  ],\n",
       "       [-0.02,  0.08, -0.04, -0.02],\n",
       "       [ 0.  ,  0.02, -0.04,  0.02],\n",
       "       [ 0.02,  0.02, -0.08,  0.04],\n",
       "       [-0.02,  0.  ,  0.02,  0.  ],\n",
       "       [ 0.04,  0.  , -0.04,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.02,  0.  ,  0.02, -0.04],\n",
       "       [-0.02,  0.04, -0.02,  0.  ],\n",
       "       [ 0.04, -0.02,  0.02, -0.04],\n",
       "       [ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [ 0.02, -0.04, -0.02,  0.04],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [-0.02, -0.02,  0.04,  0.  ],\n",
       "       [-0.04,  0.02,  0.  ,  0.02],\n",
       "       [-0.04, -0.02,  0.06,  0.  ],\n",
       "       [ 0.  ,  0.02, -0.02,  0.  ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = pd.read_csv(r'C:/Users/12239/Desktop/CFF/离散制造过程中典型工件的质量符合率预测/submit_example0.694attr.csv')\n",
    "submit[ratio].values - compare[ratio].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  , -0.02,  0.02,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [-0.02,  0.  ,  0.02,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.02, -0.02,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.02,  0.  , -0.02],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.02, -0.02,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  , -0.02,  0.02,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  , -0.02,  0.  ,  0.02],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.02, -0.02,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  , -0.02,  0.  ,  0.02],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [-0.02,  0.  ,  0.  ,  0.02],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.02, -0.02,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.02,  0.  , -0.02,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [-0.02,  0.  ,  0.  ,  0.02],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  , -0.02,  0.02],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare1 = pd.read_csv(r'C:/Users/12239/Desktop/CFF/离散制造过程中典型工件的质量符合率预测/submit.csv')\n",
    "submit[ratio].values - compare1[ratio].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\submit_example.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-a211e35b2113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0max1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbin_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Parameter5'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Parameter5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bin_label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1810\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[0;32m   4180\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4182\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y must be the same size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "ax1.scatter(train[bin_label],train['Parameter5'],color='b',label='3',marker='.')\n",
    "ax1.set_ylabel('Parameter5')\n",
    "ax1.set_xlabel('bin_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'featexp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-1fbc3bcc7ad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfeatexp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'featexp'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
