{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cbt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,log_loss,precision_score, recall_score, f1_score \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime,timedelta\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\train_set\\first_round_training_data - 副本.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\test_set\\first_round_testing_data - 副本.csv')\n",
    "submit = pd.read_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\submit_example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_map = {'Excellent': 0, 'Good': 1, 'Pass': 2, 'Fail': 3}\n",
    "train['label'] = train['Quality_label'].map(quality_map)\n",
    "btype = 'lgb'\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：-0.188\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：-0.263\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：-0.187\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-0.229\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-0.049\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：-0.033\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-4610.139\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-1249.316\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-1597.847\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-1055.412\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：-2.683\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-3.294\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-3.827\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：-2.985\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-1.145\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：-0.005\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-0.007\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-4.262\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：-0.002\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：0.009\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-0.867\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-1.173\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-2.771\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：-0.704\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "最佳效果：-0.099\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "mae 44358570.085512206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "parameter_name = ['Parameter{0}'.format(i) for i in range(5,11)]\n",
    "attribute_name = ['Attribute{0}'.format(i) for i in range(4,9)]\n",
    "X_train_para = train[parameter_name]\n",
    "y_train_attr = train[attribute_name]\n",
    "X_test_para = test[parameter_name]\n",
    "y_train = train['label']\n",
    "train_attr = np.zeros((X_train_para.shape[0],5))\n",
    "test_attr = np.zeros((X_test_para.shape[0],5))\n",
    "i = 0\n",
    "\n",
    "for attr in attribute_name:\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=2019)\n",
    "    for train_index, test_index in kfold.split(X_train_para,y_train_attr):\n",
    "        train_x, test_x, train_y, test_y = X_train_para.iloc[train_index], X_train_para.iloc[test_index], y_train_attr[attr].iloc[train_index], y_train_attr[attr].iloc[test_index]\n",
    "        if btype == 'cbt':\n",
    "            pipeline = Pipeline([('attr_model', CatBoostRegressor(random_state=2019))])\n",
    "        elif btype == 'lgb':\n",
    "            pipeline = Pipeline([('attr_model', LGBMRegressor(boosting_type=\"gbdt\",random_state=2019))])\n",
    "        elif btype == 'xgb':\n",
    "            pipeline = Pipeline([('attr_model', XGBRegressor(random_state=2019))])\n",
    "\n",
    "        n_estimators = range(500,1200,100)\n",
    "        max_depth = [4,6,8,10]\n",
    "        reg_alpha=[5,7,9,11,13]\n",
    "        reg_lambda=[3,4,5,6]\n",
    "        learning_rate=[0.01,0.03,0.05]\n",
    "        random_grid = {\n",
    "                        'attr_model__n_estimators': n_estimators,\n",
    "                        'attr_model__max_depth': max_depth,\n",
    "                        'attr_model__reg_alpha':reg_alpha,\n",
    "                        'attr_model__reg_lambda':reg_lambda,\n",
    "                        'attr_model__learning_rate':learning_rate\n",
    "                       } \n",
    "        grid_search = GridSearchCV(pipeline,random_grid, n_jobs=-1, verbose=1, scoring=None)\n",
    "        grid_search.fit(train_x,train_y)\n",
    "        print('最佳效果：%0.3f' % grid_search.best_score_)\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        print('最优参数：')\n",
    "        for param_name in sorted(random_grid.keys()):\n",
    "            print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "        train_attr[test_index,i] += grid_search.predict(test_x)\n",
    "        test_attr[:,i] += grid_search.predict(X_test_para)/k\n",
    "    i += 1\n",
    "print('mae',np.sum(np.absolute(train_attr - y_train_attr.values)/480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 11) (6000, 11)\n",
      "1\n",
      "0\n",
      "0:\tlearn: -1.3810173\ttest: -1.3810173\tbest: -1.3810173 (0)\ttotal: 15.9ms\tremaining: 143ms\n",
      "9:\tlearn: -1.3410769\ttest: -1.3410768\tbest: -1.3410768 (9)\ttotal: 153ms\tremaining: 0us\n",
      "bestTest = -1.341076767\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "1\n",
      "0:\tlearn: -1.3809371\ttest: -1.3809372\tbest: -1.3809372 (0)\ttotal: 22.8ms\tremaining: 205ms\n",
      "9:\tlearn: -1.3410162\ttest: -1.3410162\tbest: -1.3410162 (9)\ttotal: 143ms\tremaining: 0us\n",
      "bestTest = -1.341016215\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "2\n",
      "0:\tlearn: -1.3808888\ttest: -1.3808888\tbest: -1.3808888 (0)\ttotal: 15ms\tremaining: 135ms\n",
      "9:\tlearn: -1.3407111\ttest: -1.3407110\tbest: -1.3407110 (9)\ttotal: 167ms\tremaining: 0us\n",
      "bestTest = -1.340710981\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "3\n",
      "0:\tlearn: -1.3807652\ttest: -1.3807654\tbest: -1.3807654 (0)\ttotal: 17.7ms\tremaining: 160ms\n",
      "9:\tlearn: -1.3398076\ttest: -1.3398076\tbest: -1.3398076 (9)\ttotal: 180ms\tremaining: 0us\n",
      "bestTest = -1.339807645\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "4\n",
      "0:\tlearn: -1.3808157\ttest: -1.3808159\tbest: -1.3808159 (0)\ttotal: 14.8ms\tremaining: 133ms\n",
      "9:\tlearn: -1.3399107\ttest: -1.3399107\tbest: -1.3399107 (9)\ttotal: 158ms\tremaining: 0us\n",
      "bestTest = -1.339910657\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "logloss 1.3419929230906364\n",
      "ac 0.5085\n",
      "mae 0.05141238654191438\n",
      "2\n",
      "0\n",
      "0:\tlearn: -1.3809282\ttest: -1.3809281\tbest: -1.3809281 (0)\ttotal: 17ms\tremaining: 153ms\n",
      "9:\tlearn: -1.3403048\ttest: -1.3403049\tbest: -1.3403049 (9)\ttotal: 161ms\tremaining: 0us\n",
      "bestTest = -1.340304859\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "1\n",
      "0:\tlearn: -1.3808898\ttest: -1.3808898\tbest: -1.3808898 (0)\ttotal: 14.9ms\tremaining: 134ms\n",
      "9:\tlearn: -1.3404834\ttest: -1.3404834\tbest: -1.3404834 (9)\ttotal: 162ms\tremaining: 0us\n",
      "bestTest = -1.34048336\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "2\n",
      "0:\tlearn: -1.3807674\ttest: -1.3807674\tbest: -1.3807674 (0)\ttotal: 16.4ms\tremaining: 148ms\n",
      "9:\tlearn: -1.3397345\ttest: -1.3397343\tbest: -1.3397343 (9)\ttotal: 156ms\tremaining: 0us\n",
      "bestTest = -1.339734316\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "3\n",
      "0:\tlearn: -1.3809729\ttest: -1.3809729\tbest: -1.3809729 (0)\ttotal: 22.5ms\tremaining: 203ms\n",
      "9:\tlearn: -1.3416425\ttest: -1.3416425\tbest: -1.3416425 (9)\ttotal: 174ms\tremaining: 0us\n",
      "bestTest = -1.341642488\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "4\n",
      "0:\tlearn: -1.3809019\ttest: -1.3809018\tbest: -1.3809018 (0)\ttotal: 14ms\tremaining: 126ms\n",
      "9:\tlearn: -1.3408048\ttest: -1.3408046\tbest: -1.3408046 (9)\ttotal: 178ms\tremaining: 0us\n",
      "bestTest = -1.340804552\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "logloss 1.3419694933703028\n",
      "ac 0.5135\n",
      "mae 0.05141271100302291\n",
      "3\n",
      "0\n",
      "0:\tlearn: -1.3813332\ttest: -1.3813332\tbest: -1.3813332 (0)\ttotal: 22.1ms\tremaining: 199ms\n",
      "9:\tlearn: -1.3420351\ttest: -1.3420352\tbest: -1.3420352 (9)\ttotal: 187ms\tremaining: 0us\n",
      "bestTest = -1.342035215\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "1\n",
      "0:\tlearn: -1.3811541\ttest: -1.3811543\tbest: -1.3811543 (0)\ttotal: 15.1ms\tremaining: 136ms\n",
      "9:\tlearn: -1.3414449\ttest: -1.3414449\tbest: -1.3414449 (9)\ttotal: 163ms\tremaining: 0us\n",
      "bestTest = -1.341444861\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "2\n",
      "0:\tlearn: -1.3809062\ttest: -1.3809062\tbest: -1.3809062 (0)\ttotal: 14.6ms\tremaining: 131ms\n",
      "9:\tlearn: -1.3412595\ttest: -1.3412594\tbest: -1.3412594 (9)\ttotal: 153ms\tremaining: 0us\n",
      "bestTest = -1.341259369\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "3\n",
      "0:\tlearn: -1.3808717\ttest: -1.3808717\tbest: -1.3808717 (0)\ttotal: 15.8ms\tremaining: 142ms\n",
      "9:\tlearn: -1.3404309\ttest: -1.3404310\tbest: -1.3404310 (9)\ttotal: 165ms\tremaining: 0us\n",
      "bestTest = -1.340430989\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "4\n",
      "0:\tlearn: -1.3809171\ttest: -1.3809171\tbest: -1.3809171 (0)\ttotal: 17.5ms\tremaining: 158ms\n",
      "9:\tlearn: -1.3411663\ttest: -1.3411663\tbest: -1.3411663 (9)\ttotal: 183ms\tremaining: 0us\n",
      "bestTest = -1.341166339\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "logloss 1.3427626489404874\n",
      "ac 0.5105\n",
      "mae 0.05139791174408082\n",
      "4\n",
      "0\n",
      "0:\tlearn: -1.3809140\ttest: -1.3809140\tbest: -1.3809140 (0)\ttotal: 19.8ms\tremaining: 178ms\n",
      "9:\tlearn: -1.3397661\ttest: -1.3397661\tbest: -1.3397661 (9)\ttotal: 151ms\tremaining: 0us\n",
      "bestTest = -1.339766101\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "1\n",
      "0:\tlearn: -1.3808013\ttest: -1.3808013\tbest: -1.3808013 (0)\ttotal: 14.6ms\tremaining: 131ms\n",
      "9:\tlearn: -1.3402072\ttest: -1.3402072\tbest: -1.3402072 (9)\ttotal: 150ms\tremaining: 0us\n",
      "bestTest = -1.340207162\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "2\n",
      "0:\tlearn: -1.3807326\ttest: -1.3807326\tbest: -1.3807326 (0)\ttotal: 22.4ms\tremaining: 202ms\n",
      "9:\tlearn: -1.3392307\ttest: -1.3392309\tbest: -1.3392309 (9)\ttotal: 156ms\tremaining: 0us\n",
      "bestTest = -1.339230881\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "3\n",
      "0:\tlearn: -1.3808885\ttest: -1.3808883\tbest: -1.3808883 (0)\ttotal: 18.2ms\tremaining: 164ms\n",
      "9:\tlearn: -1.3399561\ttest: -1.3399561\tbest: -1.3399561 (9)\ttotal: 184ms\tremaining: 0us\n",
      "bestTest = -1.339956133\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "4\n",
      "0:\tlearn: -1.3810433\ttest: -1.3810433\tbest: -1.3810433 (0)\ttotal: 20.1ms\tremaining: 181ms\n",
      "9:\tlearn: -1.3422730\ttest: -1.3422731\tbest: -1.3422731 (9)\ttotal: 151ms\tremaining: 0us\n",
      "bestTest = -1.342273056\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "logloss 1.3420149148748666\n",
      "ac 0.5125\n",
      "mae 0.05141218586095186\n",
      "5\n",
      "0\n",
      "0:\tlearn: -1.3808201\ttest: -1.3808201\tbest: -1.3808201 (0)\ttotal: 21.6ms\tremaining: 194ms\n",
      "9:\tlearn: -1.3394420\ttest: -1.3394420\tbest: -1.3394420 (9)\ttotal: 163ms\tremaining: 0us\n",
      "bestTest = -1.339441971\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "1\n",
      "0:\tlearn: -1.3808795\ttest: -1.3808795\tbest: -1.3808795 (0)\ttotal: 14.6ms\tremaining: 131ms\n",
      "9:\tlearn: -1.3412278\ttest: -1.3412278\tbest: -1.3412278 (9)\ttotal: 154ms\tremaining: 0us\n",
      "bestTest = -1.34122779\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "2\n",
      "0:\tlearn: -1.3808270\ttest: -1.3808272\tbest: -1.3808272 (0)\ttotal: 15.6ms\tremaining: 140ms\n",
      "9:\tlearn: -1.3401865\ttest: -1.3401865\tbest: -1.3401865 (9)\ttotal: 153ms\tremaining: 0us\n",
      "bestTest = -1.340186493\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "3\n",
      "0:\tlearn: -1.3808120\ttest: -1.3808118\tbest: -1.3808118 (0)\ttotal: 15.1ms\tremaining: 136ms\n",
      "9:\tlearn: -1.3391755\ttest: -1.3391755\tbest: -1.3391755 (9)\ttotal: 151ms\tremaining: 0us\n",
      "bestTest = -1.339175453\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "4\n",
      "0:\tlearn: -1.3809959\ttest: -1.3809961\tbest: -1.3809961 (0)\ttotal: 13.7ms\tremaining: 123ms\n",
      "9:\tlearn: -1.3419685\ttest: -1.3419685\tbest: -1.3419685 (9)\ttotal: 154ms\tremaining: 0us\n",
      "bestTest = -1.341968516\n",
      "bestIteration = 9\n",
      "Shrink model to first 10 iterations.\n",
      "logloss 1.3418902228774403\n",
      "ac 0.5086666666666667\n",
      "mae 0.05141436800639568\n",
      "logloss 1.3421051089172642\n",
      "ac 0.5118333333333334\n",
      "mae 0.05140991191926527\n",
      "micro precision 0.5118333333333334\n",
      "micro recall 0.5118333333333334\n",
      "macro precision 0.49487724585060533\n",
      "macro recall 0.44013176836890344\n",
      "micro F2-Score 0.5118333333333334\n",
      "macro F2-Score 0.450089950713362\n"
     ]
    }
   ],
   "source": [
    "train[attribute_name] = train_attr\n",
    "X_train = train[parameter_name+attribute_name]\n",
    "y_train = train['label']\n",
    "i = 0\n",
    "for attr in attribute_name:\n",
    "    test[attr] = test_attr[:,i]\n",
    "    i += 1\n",
    "X_test = test[parameter_name + attribute_name]\n",
    "print(X_train.shape,X_test.shape)\n",
    "oof1 = np.zeros((X_train.shape[0],4))\n",
    "prediction1 = np.zeros((X_test.shape[0],4))\n",
    "seeds = [x*x for x in range(10,20,2)]#设置随机种子\n",
    "num_model_seed = len(seeds)\n",
    "#params = {'max_depth':range(2, 7), 'n_estimators':range(100, 1100, 200), 'learning_rate':[0.01]}\n",
    "\n",
    "for model_seed in range(num_model_seed):\n",
    "    print(model_seed + 1)\n",
    "    oof1_model = np.zeros((X_train.shape[0],4))\n",
    "    prediction1_model=np.zeros((X_test.shape[0],4))\n",
    "    skf = StratifiedKFold(n_splits=k, random_state=seeds[model_seed], shuffle=True)#构造交叉验证集，进行按比例取样（splits表示测试集和验证集为4：1）\n",
    "    for index, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(index)\n",
    "        train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        gc.collect()#进行垃圾内存回收\n",
    "        model1 = cbt.CatBoostClassifier(iterations=1500,learning_rate=0.01,max_depth=6,verbose = 500,\n",
    "                                           leaf_estimation_method='Newton',early_stopping_rounds=10000,\n",
    "                                           task_type='GPU',loss_function='MultiClass')\n",
    "        model1.fit(train_x, train_y ,eval_set=(train_x, train_y))\n",
    "        oof1_model[test_index] = oof1_model[test_index]+model1.predict_proba(test_x) #预测验证集上某一个工件lebal为0到3的各自概率\n",
    "        prediction1_model = prediction1_model+model1.predict_proba(X_test)/k #预测测试集上某一个工件lebal为0到3的各自概率（除10是因为splits=10，故要进行10次轮换交叉验证）\n",
    "    oof1 = oof1+oof1_model / num_model_seed\n",
    "    prediction1 =prediction1+prediction1_model / num_model_seed\n",
    "\n",
    "    print('logloss',log_loss(pd.get_dummies(y_train).values, oof1_model))\n",
    "    print('ac',accuracy_score(y_train, np.argmax(oof1_model,axis=1)))\n",
    "    print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y_train] - oof1_model))/480))\n",
    "\n",
    "print('logloss',log_loss(pd.get_dummies(y_train).values, oof1))\n",
    "print('ac',accuracy_score(y_train, np.argmax(oof1,axis=1)))\n",
    "print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y_train] - oof1))/480))\n",
    "beta=2\n",
    "p1=precision_score(y_train,np.argmax(oof1,axis=1),average='micro')\n",
    "r1=recall_score(y_train,np.argmax(oof1,axis=1),average='micro')\n",
    "p2=precision_score(y_train,np.argmax(oof1,axis=1),average='macro')\n",
    "r2=recall_score(y_train,np.argmax(oof1,axis=1),average='macro')\n",
    "print('micro precision',p1)\n",
    "print('micro recall',r1)\n",
    "print('macro precision',p2)\n",
    "print('macro recall',r2)\n",
    "print('micro F2-Score',(1+beta*beta)*p1*r1/(beta*beta*p1+r1))\n",
    "print('macro F2-Score',(1+beta*beta)*p2*r2/(beta*beta*p2+r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 11) (6000, 11)\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.243252484658414\n",
      "ac 0.5153333333333333\n",
      "mae 0.053806622071430624\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.2426384055605595\n",
      "ac 0.5136666666666667\n",
      "mae 0.05382582867836232\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.2420835262121512\n",
      "ac 0.5168333333333334\n",
      "mae 0.05383540646487394\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.2449834433817697\n",
      "ac 0.5131666666666667\n",
      "mae 0.05378109897024472\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "logloss 1.2441056435461844\n",
      "ac 0.517\n",
      "mae 0.053792503892009846\n",
      "logloss 1.2416356234234318\n",
      "ac 0.5255\n",
      "mae 0.053808284454395186\n",
      "micro precision 0.5255\n",
      "micro recall 0.5255\n",
      "macro precision 0.5005579051674008\n",
      "macro recall 0.4676025897291758\n",
      "micro F2-Score 0.5255\n",
      "macro F2-Score 0.4738418712078171\n"
     ]
    }
   ],
   "source": [
    "train[attribute_name] = train_attr\n",
    "X_train = train[parameter_name+attribute_name]\n",
    "y_train = train['label']\n",
    "i = 0\n",
    "for attr in attribute_name:\n",
    "    test[attr] = test_attr[:,i]\n",
    "    i += 1\n",
    "X_test = test[parameter_name + attribute_name]\n",
    "print(X_train.shape,X_test.shape)\n",
    "oof2 = np.zeros((X_train.shape[0],4))\n",
    "prediction2 = np.zeros((X_test.shape[0],4))\n",
    "seeds = [x*x for x in range(10,20,2)]#设置随机种子\n",
    "num_model_seed = len(seeds)\n",
    "#params = {'max_depth':range(2, 7), 'n_estimators':range(100, 1100, 200), 'learning_rate':[0.01]}\n",
    "\n",
    "for model_seed in range(num_model_seed):\n",
    "    print(model_seed + 1)\n",
    "    oof2_model = np.zeros((X_train.shape[0],4))\n",
    "    prediction2_model=np.zeros((X_test.shape[0],4))\n",
    "    skf = StratifiedKFold(n_splits=k, random_state=seeds[model_seed], shuffle=True)#构造交叉验证集，进行按比例取样（splits表示测试集和验证集为4：1）\n",
    "    for index, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(index)\n",
    "        train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        gc.collect()#进行垃圾内存回收\n",
    "        model2 = XGBClassifier(\n",
    "            silent=True ,\n",
    "            learning_rate= 0.05, # 学习率\n",
    "            min_child_weight=1,\n",
    "            # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "            #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "            #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "            max_depth=7, # 构建树的深度，越大越容易过拟合\n",
    "            gamma=0.1,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "            subsample=1, # 随机采样训练样本 训练实例的子采样比\n",
    "            max_delta_step=0,#最大增量步长，我们允许每个树的权重估计。\n",
    "            colsample_bytree=1, # 生成树时进行的列采样\n",
    "            reg_lambda=1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "            #reg_alpha=0, # L1 正则项参数\n",
    "            scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n",
    "            objective= 'multi:softmax', #多分类的问题 指定学习任务和相应的学习目标\n",
    "            num_class=4, # 类别数，多分类与 multisoftmax 并用\n",
    "            n_estimators=1800, #树的个数\n",
    "            seed=2019, #随机种子\n",
    "            eval_metric= 'mlogloss',)\n",
    "        model2.fit(train_x, train_y,eval_metric='mlogloss')\n",
    "        oof2_model[test_index] = oof2_model[test_index]+model2.predict_proba(test_x) #预测验证集上某一个工件lebal为0到3的各自概率\n",
    "        prediction2_model = prediction2_model+model2.predict_proba(X_test)/k #预测测试集上某一个工件lebal为0到3的各自概率（除10是因为splits=10，故要进行10次轮换交叉验证）\n",
    "    oof2 = oof2+oof2_model / num_model_seed\n",
    "    prediction2 =prediction2+prediction2_model / num_model_seed\n",
    "\n",
    "    print('logloss',log_loss(pd.get_dummies(y_train).values, oof2_model))\n",
    "    print('ac',accuracy_score(y_train, np.argmax(oof2_model,axis=1)))\n",
    "    print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y_train] - oof2_model))/480))\n",
    "\n",
    "print('logloss',log_loss(pd.get_dummies(y_train).values, oof2))\n",
    "print('ac',accuracy_score(y_train, np.argmax(oof2,axis=1)))\n",
    "print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y_train] - oof2))/480))\n",
    "beta=2\n",
    "p1=precision_score(y_train,np.argmax(oof2,axis=1),average='micro')\n",
    "r1=recall_score(y_train,np.argmax(oof2,axis=1),average='micro')\n",
    "p2=precision_score(y_train,np.argmax(oof2,axis=1),average='macro')\n",
    "r2=recall_score(y_train,np.argmax(oof2,axis=1),average='macro')\n",
    "print('micro precision',p1)\n",
    "print('micro recall',r1)\n",
    "print('macro precision',p2)\n",
    "print('macro recall',r2)\n",
    "print('micro F2-Score',(1+beta*beta)*p1*r1/(beta*beta*p1+r1))\n",
    "print('macro F2-Score',(1+beta*beta)*p2*r2/(beta*beta*p2+r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 11) (6000, 11)\n",
      "1\n",
      "0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23634\n",
      "1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23791\n",
      "2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23594\n",
      "3\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.2424\n",
      "4\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23828\n",
      "logloss 1.2381736343132588\n",
      "ac 0.4028333333333333\n",
      "mae 0.05499846148654473\n",
      "2\n",
      "0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.2366\n",
      "1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.24284\n",
      "2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23756\n",
      "3\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23555\n",
      "4\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23825\n",
      "logloss 1.2381626467210378\n",
      "ac 0.4028333333333333\n",
      "mae 0.05499698957683813\n",
      "3\n",
      "0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23526\n",
      "1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.24372\n",
      "2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23791\n",
      "3\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23747\n",
      "4\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23472\n",
      "logloss 1.2378198210820408\n",
      "ac 0.4028333333333333\n",
      "mae 0.05500679491689221\n",
      "4\n",
      "0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23546\n",
      "1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23882\n",
      "2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.24172\n",
      "3\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.24018\n",
      "4\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23415\n",
      "logloss 1.2380698318430754\n",
      "ac 0.4028333333333333\n",
      "mae 0.05500754795222239\n",
      "5\n",
      "0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.24167\n",
      "1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23637\n",
      "2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23897\n",
      "3\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.24135\n",
      "4\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's multi_logloss: 1.23207\n",
      "logloss 1.238087771181704\n",
      "ac 0.4028333333333333\n",
      "mae 0.0550009684205041\n",
      "logloss 1.237904824967115\n",
      "ac 0.4028333333333333\n",
      "mae 0.05500215213482033\n",
      "micro precision 0.4028333333333333\n",
      "micro recall 0.4028333333333333\n",
      "macro precision 0.10070833333333333\n",
      "macro recall 0.25\n",
      "micro F2-Score 0.4028333333333333\n",
      "macro F2-Score 0.19282933367372987\n"
     ]
    }
   ],
   "source": [
    "train[attribute_name] = train_attr\n",
    "X_train = train[parameter_name+attribute_name]\n",
    "y_train = train['label']\n",
    "i = 0\n",
    "for attr in attribute_name:\n",
    "    test[attr] = test_attr[:,i]\n",
    "    i += 1\n",
    "X_test = test[parameter_name + attribute_name]\n",
    "print(X_train.shape,X_test.shape)\n",
    "oof3 = np.zeros((X_train.shape[0],4))\n",
    "prediction3 = np.zeros((X_test.shape[0],4))\n",
    "seeds = [x*x for x in range(10,20,2)]#设置随机种子\n",
    "num_model_seed = len(seeds)\n",
    "#params = {'max_depth':range(2, 7), 'n_estimators':range(100, 1100, 200), 'learning_rate':[0.01]}\n",
    "\n",
    "for model_seed in range(num_model_seed):\n",
    "    print(model_seed + 1)\n",
    "    oof3_model = np.zeros((X_train.shape[0],4))\n",
    "    prediction3_model=np.zeros((X_test.shape[0],4))\n",
    "    skf = StratifiedKFold(n_splits=k, random_state=seeds[model_seed], shuffle=True)#构造交叉验证集，进行按比例取样（splits表示测试集和验证集为4：1）\n",
    "    for index, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(index)\n",
    "        train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        gc.collect()#进行垃圾内存回收\n",
    "        model3=LGBMClassifier(boosting_type=\"gbdt\",num_leaves=23,reg_alpha=10, reg_lambda=5,max_depth=7,\n",
    "                                 learning_rate=0.05, n_estimators=2000,colsample_bytree=0.7, subsample_freq=1,\n",
    "                                 objective='multiclass', silent=True,subsample=0.7,min_child_samples=5,)\n",
    "        model3.fit(train_x, train_y, eval_set=(test_x, test_y),early_stopping_rounds=1500, verbose=500,)#verbose=False   \n",
    "        oof3_model[test_index] = oof3_model[test_index]+model3.predict_proba(test_x) #预测验证集上某一个工件lebal为0到3的各自概率\n",
    "        prediction3_model = prediction3_model+model3.predict_proba(X_test)/k #预测测试集上某一个工件lebal为0到3的各自概率（除10是因为splits=10，故要进行10次轮换交叉验证）\n",
    "    oof3 = oof3+oof3_model / num_model_seed\n",
    "    prediction3 =prediction3+prediction3_model / num_model_seed\n",
    "\n",
    "    print('logloss',log_loss(pd.get_dummies(y_train).values, oof3_model))\n",
    "    print('ac',accuracy_score(y_train, np.argmax(oof3_model,axis=1)))\n",
    "    print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y_train] - oof3_model))/480))\n",
    "\n",
    "print('logloss',log_loss(pd.get_dummies(y_train).values, oof3))\n",
    "print('ac',accuracy_score(y_train, np.argmax(oof3,axis=1)))\n",
    "print('mae',1/(1 + np.sum(np.absolute(np.eye(4)[y_train] - oof3))/480))\n",
    "beta=2\n",
    "p1=precision_score(y_train,np.argmax(oof3,axis=1),average='micro')\n",
    "r1=recall_score(y_train,np.argmax(oof3,axis=1),average='micro')\n",
    "p2=precision_score(y_train,np.argmax(oof3,axis=1),average='macro')\n",
    "r2=recall_score(y_train,np.argmax(oof3,axis=1),average='macro')\n",
    "print('micro precision',p1)\n",
    "print('micro recall',r1)\n",
    "print('macro precision',p2)\n",
    "print('macro recall',r2)\n",
    "print('micro F2-Score',(1+beta*beta)*p1*r1/(beta*beta*p1+r1))\n",
    "print('macro F2-Score',(1+beta*beta)*p2*r2/(beta*beta*p2+r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosttype = 'lgb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳效果：0.403\n",
      "最优参数：\n",
      "\tattr_model__n_estimators: 1\n",
      "mae 9.770833333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "new_train = np.zeros((6000,12))\n",
    "new_test = np.zeros((6000,12))\n",
    "new_train[:,0:4] = oof1\n",
    "new_train[:,4:8] = oof2\n",
    "new_train[:,8:12] = oof3\n",
    "new_test[:,0:4] = prediction1\n",
    "new_test[:,4:8] = prediction2\n",
    "new_test[:,8:12] = prediction3\n",
    "if boosttype == 'cbt':\n",
    "    pipeline = Pipeline([('attr_model', CatBoostClassifier(random_state=2019))])\n",
    "elif boosttype == 'lgb':\n",
    "    pipeline = Pipeline([('attr_model', LGBMClassifier(boosting_type=\"gbdt\",random_state=2019))])\n",
    "elif boosttype == 'xgb':\n",
    "    pipeline = Pipeline([('attr_model', XGBClassifier(random_state=2019))])\n",
    "n_estimators = [1]#range(500,1200,100)\n",
    "#max_depth = [4,6,8]\n",
    "#reg_alpha=[5,7,9]\n",
    "#reg_lambda=[4,5,6]\n",
    "#learning_rate=[0.01,0.03,0.05]\n",
    "random_grid = {\n",
    "                'attr_model__n_estimators': n_estimators,\n",
    "                #'attr_model__max_depth': max_depth,\n",
    "                #'attr_model__reg_alpha':reg_alpha,\n",
    "                #'attr_model__reg_lambda':reg_lambda,\n",
    "                #'attr_model__learning_rate':learning_rate\n",
    "                } \n",
    "grid_search = GridSearchCV(pipeline,random_grid, n_jobs=-1, verbose=1, scoring=None)\n",
    "grid_search.fit(new_train,train['label'].values)\n",
    "print('最佳效果：%0.3f' % grid_search.best_score_)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "print('最优参数：')\n",
    "for param_name in sorted(random_grid.keys()):\n",
    "    print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "prediction = grid_search.predict_proba(new_test)\n",
    "print('mae',np.sum(np.absolute(np.argmax(prediction,axis=1) - train['label'].values)/480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test[['Group']]\n",
    "prob_cols = [i for i in submit.columns if i not in ['Group']]\n",
    "for i, f in enumerate(prob_cols):\n",
    "    sub[f] = prediction[:, i]\n",
    "for i in prob_cols:\n",
    "    sub[i] = sub.groupby('Group')[i].transform('mean')\n",
    "sub = sub.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = sub.copy()\n",
    "submit2 = sub.copy()\n",
    "submit3 = sub.copy()\n",
    "ratio = ['Excellent ratio','Good ratio','Pass ratio','Fail ratio']\n",
    "submit[ratio]=submit[ratio]*50\n",
    "submit2[ratio]=(submit2[ratio]*50).astype('int')\n",
    "submit3[ratio]=(submit[ratio]-submit2[ratio]).astype('float')\n",
    "submit2 = submit2.values\n",
    "submit3 = submit3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.argmax(submit3[:,1:5],axis=1) + 1\n",
    "for i in range(120): \n",
    "    if submit3[i,k[i]] > 0.5:\n",
    "        submit2[i,k[i]] += 1\n",
    "        submit3[i,k[i]] = 0\n",
    "k = np.argmax(submit3[:,1:5],axis=1) + 1\n",
    "for i in range(120):\n",
    "    if submit3[i,k[i]] > 0.5:\n",
    "        submit2[i,k[i]] += 1\n",
    "        submit3[i,k[i]] = 0\n",
    "k = np.argmax(submit3[:,1:5],axis=1) + 1\n",
    "for i in range(120):    \n",
    "    if sum(submit2[i,1:5]) < 50:\n",
    "        if submit3[i,k[i]] > 0.5:\n",
    "            submit2[i,k[i]] += 1\n",
    "            submit3[i,k[i]] = 0\n",
    "k = np.argmax(submit3[:,1:5],axis=1) + 1\n",
    "for i in range(120):    \n",
    "    if sum(submit2[i,1:5]) < 50:\n",
    "        submit2[i,k[i]] += 1\n",
    "        submit3[i,k[i]] = 0\n",
    "submit[ratio] = submit2[:,1:5] / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(r'C:\\Users\\12239\\Desktop\\CFF\\离散制造过程中典型工件的质量符合率预测\\submit_example.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
